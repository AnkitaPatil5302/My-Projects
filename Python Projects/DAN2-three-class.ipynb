{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from scipy.optimize import minimize_scalar\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils.extmath import safe_sparse_dot\n",
    "from time import strftime, gmtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DAN2Regressor(object):\n",
    "\n",
    "    def __init__(self, depth=10, bounds=(0,5000000000)):\n",
    "        self.bounds = bounds\n",
    "        self.depth = depth\n",
    "        self.lin_predictor = LinearRegression(fit_intercept=True)\n",
    "        self.coef_ = None\n",
    "        self.name = strftime('dan2model-'+ str(depth) + '-%Y-%b-%d-%H-%M-%S', gmtime())\n",
    "        #self.lin_predictions = None\n",
    "\n",
    "\n",
    "    \"\"\" Layer activation \"\"\"\n",
    "    def f(self, x):\n",
    "        \n",
    "        f = self.f_k\n",
    "        A = self.A\n",
    "        alpha = self.alpha\n",
    "        a = self.a\n",
    "        rows = f.shape[0]\n",
    "        ''' check if intercept term should be placed first'''\n",
    "        #Xn = np.hstack((a, A[0]*f, A[1]*np.cos(alpha*x), A[2]*np.sin(alpha*x)))\n",
    "        Xn = a + A[0]*f + A[1]*np.cos(alpha*x) + A[2]*np.sin(alpha*x)\n",
    "        return np.sum(Xn)\n",
    "\n",
    "\n",
    "    \"\"\" Method to get alpha column for DAN2 \"\"\"\n",
    "    def compute_alpha(self, X):\n",
    "        cols = X.shape[1]\n",
    "\n",
    "        \"\"\" Create resultant vector of ones \"\"\"\n",
    "        R = np.ones(cols)\n",
    "        #print('R', R.shape)\n",
    "\n",
    "        \"\"\" Compute dot product \"\"\"\n",
    "        X_dot_R = (1 + np.dot(X,R))\n",
    "        #print('XdR', X_dot_R.shape)\n",
    "        X_dot_R = X_dot_R.reshape((len(X),))\n",
    "        #print('XdR', X_dot_R.shape)\n",
    "\n",
    "        \"\"\" Compute X and R magnitudes \"\"\"\n",
    "        X_mag = np.sqrt(1*1 + np.sum(np.square(X), axis=1))\n",
    "        R_mag = np.sqrt(np.sum(R**2) + 1*1)\n",
    "\n",
    "        \"\"\" Compute arccosine \"\"\"\n",
    "        acos = np.arccos(X_dot_R / (X_mag * R_mag))\n",
    "        #print('acos', acos.shape)\n",
    "\n",
    "        return acos.reshape(len(acos),1) \n",
    "\n",
    "\n",
    "    \"\"\" Linear method \"\"\"\n",
    "    def linear_reg(self, X, y):\n",
    "        self.model['lr'] = LinearRegression(fit_intercept=True).fit(X, y)\n",
    "        return self.model['lr'].predict(X), self.model['lr'].coef_[0], self.model['lr'].intercept_\n",
    "\n",
    "\n",
    "    ''' '''\n",
    "    def build_X1(self, f, alpha):\n",
    "        return np.column_stack((f, np.cos(alpha), np.sin(alpha)))\n",
    "\n",
    "\n",
    "    ''' '''\n",
    "    def build_Xn(self, f, A, alpha, mu):\n",
    "        rows = f.shape[0]\n",
    "        if A is None and mu is None:\n",
    "            X = np.hstack((f, np.cos(alpha), np.sin(alpha)))\n",
    "            A = LinearRegression(fit_intercept=True).fit(X, y)\n",
    "\n",
    "        return np.hstack((A[0]*f, A[1]*np.cos(alpha*mu), A[2]*np.sin(alpha*mu)))\n",
    "\n",
    "\n",
    "    def logging(self, coef_):\n",
    "        if self.coef_ is None:\n",
    "            self.coef_ = coef_.reshape(1,5)\n",
    "\n",
    "        else:\n",
    "            self.coef_ = np.vstack((self.coef_ , coef_))\n",
    "\n",
    "\n",
    "    \"\"\" Fit method  \"\"\"\n",
    "    def fit(self, X, y):\n",
    "\n",
    "        # Number of rows\n",
    "        m = X.shape[0]\n",
    "\n",
    "        ## Get non-linear projection of input records\n",
    "        alpha = self.compute_alpha(X)\n",
    "        \n",
    "        ## Get linear model from n input cols\n",
    "        self.lin_predictor.fit(X, y)\n",
    "        f_k = self.lin_predictor.predict(X)\n",
    "        self.lin_predictions = f_k\n",
    "        \"\"\" Start fit algorithm \"\"\"\n",
    "        i = 1\n",
    "        mu = 1\n",
    "        while (i <= self.depth):\n",
    "            if i==1:\n",
    "                Xn = self.build_X1(f_k, alpha)\n",
    "                lr = LinearRegression(fit_intercept=True).fit(Xn, y)\n",
    "                A = lr.coef_[0]\n",
    "                a = lr.intercept_\n",
    "                f_k = lr.predict(Xn)\n",
    "            else:\n",
    "                mu = self.minimize(f_k, A, a, alpha)\n",
    "                Xn = self.build_Xn(f_k, A, alpha, mu) # eventually override the build_X1 method\n",
    "                lr = LinearRegression(fit_intercept=True).fit(Xn, y)\n",
    "                A = lr.coef_[0]\n",
    "                a = lr.intercept_\n",
    "                f_k = lr.predict(Xn) \n",
    "\n",
    "            # Error metrics\n",
    "            mse = self.mse(f_k, y, m)\n",
    "            pred = np.where(f_k >= 0.5, 1, 0)\n",
    "            acc = accuracy_score(y, pred)\n",
    "            \n",
    "            # Save layer\n",
    "            coef_ = A.reshape((1,3))\n",
    "            coef_ = np.insert(coef_, 0, a)\n",
    "            coef_ = np.insert(coef_, 0, mu)\n",
    "            print(i, coef_)\n",
    "            self.logging(coef_)\n",
    "\n",
    "            # add layers\n",
    "            print('Iteration:', i, \" Mu:\", mu, \"MSE:\", mse, \"Accuracy:\", acc)\n",
    "\n",
    "            i += 1\n",
    "        return f_k\n",
    "\n",
    "    def minimize(self, f_k, A, a, alpha):\n",
    "        self.f_k = f_k\n",
    "        self.A = A\n",
    "        self.alpha = alpha\n",
    "        self.a = a\n",
    "        res = minimize_scalar(self.f, bounds=self.bounds, method='bounded')\n",
    "        return res.x\n",
    "        \n",
    "\n",
    "    def mse(self, f_k, y, m):\n",
    "        return np.sum((f_k - y)**2) / m        \n",
    "\n",
    "    def _activation_function(self, X, coef_):\n",
    "        intercept = coef_[0]\n",
    "        A = coef_[1:]\n",
    "        return safe_sparse_dot(X, A.T, dense_output=True) + intercept\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        X = X_test\n",
    "        m = X.shape[0]\n",
    "        alpha = self.compute_alpha(X)\n",
    "        f_k = self.lin_predictor.predict(X)\n",
    "        i = 0\n",
    "\n",
    "        for coef_ in self.coef_:\n",
    "            mu = coef_[0]\n",
    "            if i == 0:\n",
    "                X = np.hstack((f_k, np.cos(alpha*mu), np.sin(alpha*mu)))\n",
    "                f_k = self._activation_function(X, coef_[1:])\n",
    "                f_k = f_k.reshape(m,1)\n",
    "            else:\n",
    "                X = np.hstack((prev_coef_[2]*f_k, prev_coef_[3]*np.cos(alpha*mu), prev_coef_[4]*np.sin(alpha*mu)))\n",
    "                f_k = self._activation_function(X, coef_[1:])\n",
    "                f_k = f_k.reshape(m,1)\n",
    "\n",
    "            i += 1\n",
    "            prev_coef_ = coef_\n",
    "        return f_k\n",
    "    \n",
    "    def plot_error():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import sys\n",
    "import os,string,collections\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset= pd.read_csv(r\"C:\\Users\\ajaym\\Downloads\\twitter-airline-sentiment\\Tweets2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.700000e+17</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.700000e+17</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.700000e+17</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.700000e+17</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.700000e+17</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       tweet_id         airline  \\\n",
       "0  5.700000e+17  Virgin America   \n",
       "1  5.700000e+17  Virgin America   \n",
       "2  5.700000e+17  Virgin America   \n",
       "3  5.700000e+17  Virgin America   \n",
       "4  5.700000e+17  Virgin America   \n",
       "\n",
       "                                                text  \n",
       "0                @VirginAmerica What @dhepburn said.  \n",
       "1  @VirginAmerica plus you've added commercials t...  \n",
       "2  @VirginAmerica I didn't today... Must mean I n...  \n",
       "3  @VirginAmerica it's really aggressive to blast...  \n",
       "4  @VirginAmerica and it's a really big bad thing...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc = utils.TextCleaner()\n",
    "dataset['clean_text'] = tc.transform(dataset['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re,nltk\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_tok = re.compile(f'([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])')\n",
    "\n",
    "def tokenize(s): \n",
    "    return re_tok.sub(r' \\1 ', s).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = dataset['clean_text'].apply(lambda row: tokenize(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = set(stopwords.words('english'))\n",
    "stop.update(['amp', 'rt', 'cc'])\n",
    "stop = stop - set(['no', 'not'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(row):\n",
    "    return [t for t in row if t not in stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = tokenized.apply(lambda row: remove_stopwords(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "dataset.insert(3,\"tokenized\", tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob, Word, Blobber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    (0.0, 0.0)                               \n",
       "1    (0.0, 0.0)                               \n",
       "2    (-0.3125, 0.6875)                        \n",
       "3    (0.0062500000000000056, 0.35)            \n",
       "4    (-0.3499999999999999, 0.3833333333333333)\n",
       "5    (-0.2083333333333333, 0.6333333333333333)\n",
       "6    (0.45, 0.65)                             \n",
       "7    (0.2, 0.2)                               \n",
       "8    (0.0, 0.0)                               \n",
       "9    (0.4666666666666666, 0.6)                \n",
       "Name: clean_text, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['clean_text'][:10].apply(lambda x: TextBlob(x).sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline</th>\n",
       "      <th>text</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>sentiment_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.700000e+17</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>[said]</td>\n",
       "      <td>what said</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.700000e+17</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>@VirginAmerica plus you've added commercials to the experience... tacky.</td>\n",
       "      <td>[plus, youve, added, commercials, experience, tacky]</td>\n",
       "      <td>plus youve added commercials to the experience tacky</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.700000e+17</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I need to take another trip!</td>\n",
       "      <td>[didnt, today, must, mean, need, take, another, trip]</td>\n",
       "      <td>i didnt today must mean i need to take another trip</td>\n",
       "      <td>-0.31250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.700000e+17</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp;amp; they have little recourse</td>\n",
       "      <td>[really, aggressive, blast, obnoxious, entertainment, guests, faces, little, recourse]</td>\n",
       "      <td>its really aggressive to blast obnoxious entertainment in your guests faces amp they have little recourse</td>\n",
       "      <td>0.00625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.700000e+17</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing about it</td>\n",
       "      <td>[really, big, bad, thing]</td>\n",
       "      <td>and its a really big bad thing about it</td>\n",
       "      <td>-0.35000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       tweet_id         airline  \\\n",
       "0  5.700000e+17  Virgin America   \n",
       "1  5.700000e+17  Virgin America   \n",
       "2  5.700000e+17  Virgin America   \n",
       "3  5.700000e+17  Virgin America   \n",
       "4  5.700000e+17  Virgin America   \n",
       "\n",
       "                                                                                                                             text  \\\n",
       "0  @VirginAmerica What @dhepburn said.                                                                                              \n",
       "1  @VirginAmerica plus you've added commercials to the experience... tacky.                                                         \n",
       "2  @VirginAmerica I didn't today... Must mean I need to take another trip!                                                          \n",
       "3  @VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp; they have little recourse   \n",
       "4  @VirginAmerica and it's a really big bad thing about it                                                                          \n",
       "\n",
       "                                                                                tokenized  \\\n",
       "0  [said]                                                                                   \n",
       "1  [plus, youve, added, commercials, experience, tacky]                                     \n",
       "2  [didnt, today, must, mean, need, take, another, trip]                                    \n",
       "3  [really, aggressive, blast, obnoxious, entertainment, guests, faces, little, recourse]   \n",
       "4  [really, big, bad, thing]                                                                \n",
       "\n",
       "                                                                                                  clean_text  \\\n",
       "0  what said                                                                                                   \n",
       "1  plus youve added commercials to the experience tacky                                                        \n",
       "2  i didnt today must mean i need to take another trip                                                         \n",
       "3  its really aggressive to blast obnoxious entertainment in your guests faces amp they have little recourse   \n",
       "4  and its a really big bad thing about it                                                                     \n",
       "\n",
       "   sentiment_score  \n",
       "0  0.00000          \n",
       "1  0.00000          \n",
       "2 -0.31250          \n",
       "3  0.00625          \n",
       "4 -0.35000          "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['sentiment_score'] = dataset['clean_text'].apply(lambda x: TextBlob(x).sentiment[0])\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = pd.Series([]) \n",
    "for x in range(len(dataset)):\n",
    "    if dataset[\"sentiment_score\"][x] > 0.1 :\n",
    "        sentiment[x] = \"positive\"\n",
    "        \n",
    "    elif dataset[\"sentiment_score\"][x] < (-0.1) :\n",
    "        sentiment[x] = \"negative\"\n",
    "        \n",
    "    else :\n",
    "        sentiment[x] = \"neutral\"\n",
    "        \n",
    "dataset.insert(6,\"Sentiment\", sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline</th>\n",
       "      <th>text</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.700000e+17</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>[said]</td>\n",
       "      <td>what said</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.700000e+17</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>@VirginAmerica plus you've added commercials to the experience... tacky.</td>\n",
       "      <td>[plus, youve, added, commercials, experience, tacky]</td>\n",
       "      <td>plus youve added commercials to the experience tacky</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.700000e+17</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I need to take another trip!</td>\n",
       "      <td>[didnt, today, must, mean, need, take, another, trip]</td>\n",
       "      <td>i didnt today must mean i need to take another trip</td>\n",
       "      <td>-0.31250</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.700000e+17</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp;amp; they have little recourse</td>\n",
       "      <td>[really, aggressive, blast, obnoxious, entertainment, guests, faces, little, recourse]</td>\n",
       "      <td>its really aggressive to blast obnoxious entertainment in your guests faces amp they have little recourse</td>\n",
       "      <td>0.00625</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.700000e+17</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing about it</td>\n",
       "      <td>[really, big, bad, thing]</td>\n",
       "      <td>and its a really big bad thing about it</td>\n",
       "      <td>-0.35000</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       tweet_id         airline  \\\n",
       "0  5.700000e+17  Virgin America   \n",
       "1  5.700000e+17  Virgin America   \n",
       "2  5.700000e+17  Virgin America   \n",
       "3  5.700000e+17  Virgin America   \n",
       "4  5.700000e+17  Virgin America   \n",
       "\n",
       "                                                                                                                             text  \\\n",
       "0  @VirginAmerica What @dhepburn said.                                                                                              \n",
       "1  @VirginAmerica plus you've added commercials to the experience... tacky.                                                         \n",
       "2  @VirginAmerica I didn't today... Must mean I need to take another trip!                                                          \n",
       "3  @VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp; they have little recourse   \n",
       "4  @VirginAmerica and it's a really big bad thing about it                                                                          \n",
       "\n",
       "                                                                                tokenized  \\\n",
       "0  [said]                                                                                   \n",
       "1  [plus, youve, added, commercials, experience, tacky]                                     \n",
       "2  [didnt, today, must, mean, need, take, another, trip]                                    \n",
       "3  [really, aggressive, blast, obnoxious, entertainment, guests, faces, little, recourse]   \n",
       "4  [really, big, bad, thing]                                                                \n",
       "\n",
       "                                                                                                  clean_text  \\\n",
       "0  what said                                                                                                   \n",
       "1  plus youve added commercials to the experience tacky                                                        \n",
       "2  i didnt today must mean i need to take another trip                                                         \n",
       "3  its really aggressive to blast obnoxious entertainment in your guests faces amp they have little recourse   \n",
       "4  and its a really big bad thing about it                                                                     \n",
       "\n",
       "   sentiment_score Sentiment  \n",
       "0  0.00000          neutral   \n",
       "1  0.00000          neutral   \n",
       "2 -0.31250          negative  \n",
       "3  0.00625          neutral   \n",
       "4 -0.35000          negative  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_vocab_counter(row):\n",
    "    for word in row:\n",
    "        vocab_counter[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_counter = collections.Counter()\n",
    "dataset['tokenized'].apply(update_vocab_counter);\n",
    "vocab = sorted(vocab_counter, key=vocab_counter.get, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12390"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2id = {w:i for i, w in enumerate(vocab[:max_words])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2id['unk'] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_ids(row):\n",
    "    return [w2id[w] if w in w2id else w2id['unk'] for w in row]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['tokenized_int'] = dataset['tokenized'].apply(lambda x: transform_to_ids(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = dataset['tokenized_int'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 21, 8.987704918032787)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(lens), max(lens), np.mean(lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "dataset['target'] = le.fit_transform(dataset['Sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset['tokenized_int'].values, dataset['target'].values, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in e:\\anaconda3\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: six>=1.9.0 in e:\\anaconda3\\lib\\site-packages (from keras) (1.12.0)\n",
      "Requirement already satisfied: pyyaml in e:\\anaconda3\\lib\\site-packages (from keras) (5.1)\n",
      "Requirement already satisfied: h5py in e:\\anaconda3\\lib\\site-packages (from keras) (2.9.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in e:\\anaconda3\\lib\\site-packages (from keras) (1.0.8)\n",
      "Requirement already satisfied: numpy>=1.9.1 in e:\\anaconda3\\lib\\site-packages (from keras) (1.16.2)\n",
      "Requirement already satisfied: scipy>=0.14 in e:\\anaconda3\\lib\\site-packages (from keras) (1.2.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in e:\\anaconda3\\lib\\site-packages (from keras) (1.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "x_train = pad_sequences(X_train, maxlen=maxlen, value=-1)\n",
    "x_test = pad_sequences(X_test, maxlen=maxlen, value=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,  412,  141,\n",
       "          98,  647,   -1,  129,  783,    0,   21,    5,   27],\n",
       "       [  -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1, 1348,\n",
       "        1642,    9,    9,   49,  562,   89,   55,   38,   17],\n",
       "       [  -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n",
       "         510,  274,  469,   25,   48,   99,    7, 1627,    4],\n",
       "       [  -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n",
       "          -1,   -1,   -1,   -1,   -1,   -1,   93, 4980,   -1],\n",
       "       [  -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,    0,  274,\n",
       "          24,  176, 3060,   21,   59,    0,   -1,   33,  318]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.reshape(len(y_train), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [1],\n",
       "       [1]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_fit_and_predict(training_preds, testing_preds):\n",
    "    return np.array_equal(training_preds, testing_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(X, y, depth):\n",
    "    clf = DAN2Regressor(depth=depth)\n",
    "    tr_pred = clf.fit(X, y)\n",
    "    path = clf.name\n",
    "    print(clf.coef_)\n",
    "    y_pred = clf.predict(X)\n",
    "    print(tr_pred, y_pred)\n",
    "    print(test_fit_and_predict(tr_pred, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [ 1.          0.02175377  0.86114349 -0.24256921  0.24627964]\n",
      "Iteration: 1  Mu: 1 MSE: 0.4929378992487241 Accuracy: 0.4914389799635701\n",
      "2 [ 2.53414589e+09 -3.92661536e-03  1.16560379e+00  3.10327969e-02\n",
      "  4.98088224e-02]\n",
      "Iteration: 2  Mu: 2534145892.0116167 MSE: 0.49283515190771177 Accuracy: 0.49153005464480876\n",
      "3 [ 1.12656189e+09  1.36822590e-02  8.46965382e-01 -6.09382356e-01\n",
      " -3.12614916e-01]\n",
      "Iteration: 3  Mu: 1126561886.3222744 MSE: 0.4925332630930337 Accuracy: 0.49153005464480876\n",
      "4 [3.37840729e+09 6.84786562e-04 1.18006837e+00 2.91094399e-03\n",
      " 2.13094195e-02]\n",
      "Iteration: 4  Mu: 3378407287.381178 MSE: 0.4925098072657395 Accuracy: 0.49153005464480876\n",
      "5 [ 3.20281007e+09  3.83449622e-03  8.44284623e-01 -1.06098133e+00\n",
      " -5.43135363e-01]\n",
      "Iteration: 5  Mu: 3202810068.063372 MSE: 0.4924372181826588 Accuracy: 0.49153005464480876\n",
      "6 [ 3.33354055e+09  5.17789968e-03  1.17873402e+00 -1.20464728e-02\n",
      "  2.30419904e-02]\n",
      "Iteration: 6  Mu: 3333540549.8014803 MSE: 0.4922770689947139 Accuracy: 0.4914389799635701\n",
      "7 [ 2.54881095e+09  5.92892307e-03  8.43691932e-01  5.08717441e-01\n",
      " -4.35785829e-01]\n",
      "Iteration: 7  Mu: 2548810951.7154107 MSE: 0.49220809435602936 Accuracy: 0.49153005464480876\n",
      "8 [ 1.18096836e+09  1.12305291e-03  1.18402686e+00 -6.66622332e-03\n",
      " -1.49689848e-03]\n",
      "Iteration: 8  Mu: 1180968355.1670809 MSE: 0.49220211720649076 Accuracy: 0.49153005464480876\n",
      "9 [ 2.71933371e+09  3.60096555e-03  8.41679075e-01 -1.47495838e+00\n",
      "  3.90192890e-01]\n",
      "Iteration: 9  Mu: 2719333706.592467 MSE: 0.4921528302298792 Accuracy: 0.49153005464480876\n",
      "10 [ 2.62960477e+09  4.74698811e-03  1.18296166e+00  5.62134821e-03\n",
      " -3.08048496e-02]\n",
      "Iteration: 10  Mu: 2629604773.7569823 MSE: 0.4920473090179332 Accuracy: 0.49153005464480876\n",
      "[[ 1.00000000e+00  2.17537684e-02  8.61143486e-01 -2.42569215e-01\n",
      "   2.46279640e-01]\n",
      " [ 2.53414589e+09 -3.92661536e-03  1.16560379e+00  3.10327969e-02\n",
      "   4.98088224e-02]\n",
      " [ 1.12656189e+09  1.36822590e-02  8.46965382e-01 -6.09382356e-01\n",
      "  -3.12614916e-01]\n",
      " [ 3.37840729e+09  6.84786562e-04  1.18006837e+00  2.91094399e-03\n",
      "   2.13094195e-02]\n",
      " [ 3.20281007e+09  3.83449622e-03  8.44284623e-01 -1.06098133e+00\n",
      "  -5.43135363e-01]\n",
      " [ 3.33354055e+09  5.17789968e-03  1.17873402e+00 -1.20464728e-02\n",
      "   2.30419904e-02]\n",
      " [ 2.54881095e+09  5.92892307e-03  8.43691932e-01  5.08717441e-01\n",
      "  -4.35785829e-01]\n",
      " [ 1.18096836e+09  1.12305291e-03  1.18402686e+00 -6.66622332e-03\n",
      "  -1.49689848e-03]\n",
      " [ 2.71933371e+09  3.60096555e-03  8.41679075e-01 -1.47495838e+00\n",
      "   3.90192890e-01]\n",
      " [ 2.62960477e+09  4.74698811e-03  1.18296166e+00  5.62134821e-03\n",
      "  -3.08048496e-02]]\n",
      "[[1.0882321 ]\n",
      " [1.09363855]\n",
      " [1.0861054 ]\n",
      " ...\n",
      " [1.18476766]\n",
      " [1.12822289]\n",
      " [1.11808103]] [[1.0882321 ]\n",
      " [1.09363855]\n",
      " [1.0861054 ]\n",
      " ...\n",
      " [1.18476766]\n",
      " [1.12822289]\n",
      " [1.11808103]]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main(x_train,y_train, depth=10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
